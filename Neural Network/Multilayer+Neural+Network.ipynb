{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Neural Network\n",
    "This is the Exercise for the lesson of Siraj Raval from 2017-01-20\n",
    "\n",
    "[Link to the lesson - How to Make a Neural Network - Intro to Deep Learning #2](https://www.youtube.com/watch?v=p69khggr1Jo&t=0s) on Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import exp, array, random, dot\n",
    "# The Sigmoid function, which describes an S shaped curve.\n",
    "# We pass the weighted sum of the inputs through this function to\n",
    "# normalise them between 0 and 1.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + exp(-x))\n",
    "\n",
    "# The derivative of the Sigmoid function.\n",
    "# This is the gradient of the Sigmoid curve.\n",
    "# It indicates how confident we are about the existing weight.\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer \n",
    "The layer class encouples the layer with the neuron from the network. This is a little step to make the programm more readable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, input_length, output_length):\n",
    "    \n",
    "        # We model a single neuron, with input connections of input_length and output connections of output_length.\n",
    "        # We assign random weights to a [i] x [o] matrix, with values in the range -1 to 1\n",
    "        # and mean 0.\n",
    "        self.__weights = 2 * random.random((input_length, output_length)) - 1\n",
    "        pass\n",
    "\n",
    "    \n",
    "    # The neural network thinks.\n",
    "    def think(self, inputs):\n",
    "        # Pass inputs through our neural network (our single neuron).\n",
    "        return sigmoid(dot(inputs, self.__weights))\n",
    "        \n",
    "    # adjust the weights\n",
    "    def adjust_weights(self, adjust):\n",
    "        self.__weights += adjust\n",
    "    \n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.__weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network\n",
    "The neural network allowes you in this implementation to set up the size of the input, output and the hidden layer. The algorithm in the init function uses the information to build up the outputs and inputs for the hidden layer. The output size of one layer is the input size for the next and so on. \n",
    "\n",
    "The think method is the prediction method from the video on youtube. So I didn't change the name, but include the mechanic to call the hidden layers. \n",
    "### The Backpropagation\n",
    "The backpropagation algorithm need a propagate phase where it get the results for each layer. This is implemented in the propagate method and called in the train method.\n",
    "\n",
    "After propagate and the calculation of the prediction, the backpropagate phase starts. There are many comments and I hope it is understandable. You have to calculate the error, the delta and adjust them on the layer. \n",
    "\n",
    "The post of Andrew Trask [A Neural Network in 13 lines of Python (Part 2 - Gradient Descent)](https://iamtrask.github.io/2015/07/27/python-network-part2/ \"Improving our neural network by optimizing Gradient Descent\") explaines this very well. \n",
    "\n",
    "I removed the loop for iteration on the same data in this method. This must be done by hand. In normaly, you get a bunch of data and iterate \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, input_length=3, hidden_layers=[8], output_length=1):\n",
    "        # fill in layers to the neural network\n",
    "        # Seed the random number generator, so it generates the same numbers\n",
    "        # every time the program runs.\n",
    "        random.seed(1)\n",
    "        # store the layes an an array\n",
    "        self.__hidden_layers = []\n",
    "        # the layer algorithm stores a layer into with input and output nodes\n",
    "        layer_input = input_length\n",
    "        # Build up hidden layers\n",
    "        for layer_output in hidden_layers:\n",
    "            # add new layers\n",
    "            self.__hidden_layers.append(self.__layer(layer_input, layer_output))\n",
    "            # the output of the previous layer is the input of the next layer\n",
    "            layer_input = layer_output\n",
    "        # define an output layer with the output length\n",
    "        self.__hidden_layers.append(self.__layer(layer_input, output_length))\n",
    "        pass\n",
    "    \n",
    "    def __layer(self, input_length, output_length):\n",
    "        return Layer(input_length, output_length)\n",
    "    \n",
    "    def last_layer(self):\n",
    "        return self.__hidden_layers[-1].get_weights()\n",
    "    \n",
    "    # prediction function\n",
    "    def think(self, inputs):\n",
    "        for layer in self.__hidden_layers:\n",
    "            inputs = layer.think(inputs)\n",
    "        return inputs\n",
    "    \n",
    "    \n",
    "    # propagation step\n",
    "    # move forward in the network -->\n",
    "    def __propagate(self, inputs):\n",
    "        outputs = [inputs]\n",
    "        for layer in self.__hidden_layers:\n",
    "            inputs = layer.think(inputs)\n",
    "            outputs.append(inputs)\n",
    "        return outputs\n",
    "    \n",
    "    # backpropagation\n",
    "    # get the error and the delta of this error\n",
    "    # adjust the network in the backward direction <--\n",
    "    def __backpropagation(self, training_set_outputs, propagations):\n",
    "        # layers for backpropagation\n",
    "        layers = self.__hidden_layers\n",
    "        len_layers = len(layers)\n",
    "        \n",
    "        # Backpropagation\n",
    "        deltas = [None]*(len_layers)\n",
    "\n",
    "        # calculate the error from the output layer and the delta\n",
    "        error = training_set_outputs - propagations[-1]\n",
    "        deltas[-1] = error * sigmoid_derivative(propagations[-1])\n",
    "        # in one loop, we can now calculate the error and the delta for the next iteration\n",
    "        for position in range(len_layers-1, 0, -1):\n",
    "            error = dot(deltas[position], layers[position].get_weights().T)\n",
    "            deltas[position-1] = error * sigmoid_derivative(propagations[position])\n",
    "        # after calculation of the deltas, we adjust them on the layer\n",
    "        for position in range(len_layers):\n",
    "            layers[position].adjust_weights(dot(propagations[position].T, deltas[position]))\n",
    "        \n",
    "    # train the network with backpropagation\n",
    "    def train(self, training_set_inputs, training_set_outputs):\n",
    "        \n",
    "        # propagation phase\n",
    "        propagations = self.__propagate(training_set_inputs)\n",
    "\n",
    "        # Backpropagation\n",
    "        self.__backpropagation(training_set_outputs, propagations)\n",
    "            \n",
    "        # End for\n",
    "    # End Method train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# init the network with the default parameter\n",
    "neural_network = NeuralNetwork()\n",
    "\n",
    "# The training set. We have 4 examples, each consisting of 3 input values\n",
    "# and 1 output value.\n",
    "training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n",
    "training_set_outputs = array([[0, 1, 1, 0]]).T\n",
    "\n",
    "\n",
    "print(\"New synaptic weights after training: \")\n",
    "print(neural_network.last_layer())\n",
    "\n",
    "# Train the neural network using a training set.\n",
    "# Do it 10,000 times and make small adjustments each time.\n",
    "for i in range(0, 1000):\n",
    "    neural_network.train(training_set_inputs, training_set_outputs)\n",
    "\n",
    "    \n",
    "print(\"New synaptic weights after training: \")\n",
    "print(neural_network.last_layer())\n",
    "\n",
    "# Test the neural network with a new situation.\n",
    "print(\"Considering new situation [1, 0, 0] -> ?: \")\n",
    "print(neural_network.think(array([1, 0, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
